<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence</title>
  <link rel="icon" type="image/x-icon" href="">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/diankun-wu/">Diankun Wu</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="https://liuff19.github.io/">Fangfu Liu</a><sup>1*</sup>,</span>
              <span class="author-block">
                  <a href="">Yi-Hsin Hung</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://duanyueqi.github.io/">Yueqi Duan</a><sup>1†</sup>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Tsinghua University</span>,
                    <!-- <span class="author-block">Institution Name<br>Conferance name and year</span> -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>,
                    <span class="eql-cntrb"><small><sup>†</sup>Indicates Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2503.18942" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/liuff19/Video-T1" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2503.18942" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video> -->
      <img src="static/images/teaser-spatialmllm.png" alt="Teaser"/>
      <h2 class="subtitle has-text-justified">
        <strong>Spatial-MLLM:</strong> We propose Spatial-MLLM, a method that significantly enhances the visual-based spatial intelligence of existing video MLLMs. As shown, Spatial-MLLM can understand and reason about the underlying scene based on video input and achieves SOTA performance in a wide range of spatial reasoning tasks.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs, which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a spatial encoder—initialized from the backbone of the visual geometry model—to extract 3D structure feature. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct Spatial-MLLM-120k dataset and train the model using supervised-fining and GRPO on it. Extensive experiments on various real-world datasets demonstrate that our spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Concurrent Work. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method Overview</h2>
        <div class="content has-text-justified">
          <img src="./static/images/pipeline-spatialmllm-2.png">

          <p><strong>Pipeline of Spatial-MLLM.</strong> Our model is composed of a 2D visual encoder \mathcal{E}_{\text{2D}}, a spatial encoder \( \mathcal{E}_{\text{Spatial}} \) initialized from a feed-forward visual geometry foundation model, a connector module, and a large language model (LLM) backbone. At inference time, we incorporate a space-aware frame sampling strategy to ensure the model focuses on frames most critical for spatial reasoning when the input frame number is limited.</p>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Concurrent Work. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Test-Time Scaling</h2>
        <div style="display: flex; flex-direction: column">
          <div class="content has-text-justified">
            <img src="./static/images/figure.png">

            <p><strong>Test-Time Scaling for Video Generation.</strong>
              As the number of samples in the search space increases by scaling test-time computation (TTS), the models' performance exhibits consistent improvement (In the bar chart, light colors correspond to the results without TTS, while dark colors represent the improvement after TTS.). </p>

          </div>
          <br>
          <div class="content has-text-justified">
            <img src="./static/images/fig1.png">

            <p><strong>TTS consistently yields stable performance gains across different video generation models.</strong>
              We conducted a series of random linear search experiments across multiple video generation models using different verifiers. 
              Experiment results demonstrate that as the inference computational budget increases, all video generation models exhibit improved performance across different verifiers, eventually approaching a convergence limit once a certain threshold is reached. 
              This finding indicates that the TTS strategy can effectively guide the search process during test time and significantly enhance generation quality. 
              Moreover, when comparing different verifiers applied to the same video model, we observe varying growth rates and extents in their performance curves. 
              This divergence suggests that each verifier emphasizes different evaluation aspects. </p>

          </div>
          <br>
          <div class="content has-text-justified">
            <img src="./static/images/fig3.png">

            <p><strong>Performance across most dimensions can be greatly improved with TTS.</strong>
              The complexity of prompts across diverse benchmark dimensions is a key component in video TTS. We conduct experiments to quantitatively evaluate the performance improvement of different models using TTS methods across various dimensions. 
              As ToF and random linear search can achieve a similar convergence score during test-time scaling, we choose the better score for (+TTS).
              We find that for common prompt sets (<i>e.g.</i>, Scene, Object) and easily assessable categories (<i>e.g.</i>, Imaging Quality), TTS methods achieve significant improvements across different models. </p>

          </div>
          <br>
          <div class="content has-text-justified">
            <img src="./static/images/table.png">
            <p><strong>A few dimensions heavily rely on the capabilities of foundation models, making improvements challenging for TTS.</strong>
              However, for some hard-to-evaluate latent properties (<i>e.g.</i>, Motion Smoothness, Temporal Flickering), the improvement is less pronounced. 
              This is likely because Motion Smoothness requires precise control of motion trajectories across frames, which is challenging for current video generation models to achieve. 
              Temporal Flickering, on the other hand, involves maintaining consistent appearance and intensity over time, which is difficult to precisely assess, especially when dealing with complex scenes and dynamic objects.</p>

          </div>
          <br>
          <div class="content has-text-justified">
            <img src="./static/images/eval_VSIbench.png">
            <p><strong>Evaluation results on VSI-Bench.</strong>For Spatial-MLLM and Qwen2.5VL-series, we use 16 frames as input. For other open-source methods and GPT-4o, we follow the setting of VSI-Bench to set frame numbers (ranging from 8 to 32 frames). For Gemini-1.5 Pro, it samples video frames at 1 FPS. <strong>Bold</strong> and <span style="text-decoration: underline;">underline</span> denote the best-performing and second-best-performing open-source models, respectively.</p>
          </div>
          <div class="content has-text-justified">
            <img src="./static/images/eval_scanqa_sqa3d.png">
            <p><strong>Evaluation results on ScanQA and SQA3D.</strong> We use the val set of ScanQA and test set of SQA3D for evaluation following common practice~\cite{Huang2023ChatSceneB3, Huang2023AnEG, Zheng2024Video3DLL}. <strong>Bold</strong> and <span style = "text-decoration: underline;">underline</span> denote the best-performing and second-best-performing models in each model category, respectively.</p>
          </div>
          <div class="content has-text-justified">
            <img src="./static/images/train_curve.png">
            <p><strong>Visualization of Training Curves in the SFT and RL Stages.</strong> For the SFT stage, we present the
mean token accuracy and loss curves. For the RL stage, we show the dynamics of completion length and reward.</p>
          </div>
          <div class="content has-text-justified">
            <img src="./static/images/sampling-2.png">
            <p><strong>Visualization of different frame sampling strategies.</strong> For clarity of visualization, we set N<sub>m</sub> =128 and N<sub>k</sub> = 8 in this example. Uniform frame sampling often overlooks transient regions that appear briefly in the video. Furthermore, when the camera remains static for extended periods, this strategy tends to yield redundant viewpoints. In contrast, our proposed space-aware frame sampling strategy achieves more comprehensive spatial coverage.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Demos: Autoregressive Models</h2>
      <div style="display: flex">
        <div style="display: inline-block; width: 50%; padding-top: 20px; margin-bottom: -20px; text-align: center; font-size: 1.2em;">
          <strong><span style="color: red;">Left</span>: Original Video</strong>
        </div>
        <div style="display: inline-block; width: 50%; padding-top: 20px; margin-bottom: -20px; text-align: center; font-size: 1.2em;">
          <strong><span style="color: green;">Right</span>: With Test-Time Scaling</strong>
        </div>
      </div>
      <div id="results-carousel" class="carousel results-carousel"> 
        <div class="item">
          <div class="item-video">
            <video poster="" id="video3" autoplay controls muted loop class="video-left">
              <source src="static/videos/flux/A_cat_wearing_sunglasses_and_working_as_a_lifeguard_at_a_pool.mp4"
              type="video/mp4">
            </video>

            <video poster="" id="video4" autoplay controls muted loop class="video-right">
              <source src="static/videos/flux/A_cat_wearing_sunglasses_and_working_as_a_lifeguard_at_a_pool_TTS.mp4"
              type="video/mp4">
            </video>
          </div>
          <h2 class="subtitle has-text-centered">
            A cat wearing sunglasses and working as a lifeguard at a pool.
          </h2>
        </div>

        <div class="item">
          <div class="item-video">
            <video poster="" id="video3" autoplay controls muted loop class="video-left">
              <source src="static/videos/flux/a_sheep_and_a_cow.mp4"
              type="video/mp4">
            </video>

            <video poster="" id="video4" autoplay controls muted loop class="video-right">
              <source src="static/videos/flux/a_sheep_and_a_cow_TTS.mp4"
              type="video/mp4">
            </video>
          </div>
          <h2 class="subtitle has-text-centered">
            A sheep and a cow.
          </h2>
        </div>

        <div class="item">
          <div class="item-video">
            <video poster="" id="video3" autoplay controls muted loop class="video-left">
              <source src="static/videos/flux/A_cute_raccoon_playing_guitar_in_a_boat_on_the_ocean.mp4"
              type="video/mp4">
            </video>

            <video poster="" id="video4" autoplay controls muted loop class="video-right">
              <source src="static/videos/flux/A_cute_raccoon_playing_guitar_in_a_boat_on_the_ocean_TTS.mp4"
              type="video/mp4">
            </video>
          </div>
          <h2 class="subtitle has-text-centered">
            A cute raccoon playing guitar in a boat on the ocean.
          </h2>
        </div>

        <div class="item">
          <div class="item-video">
            <video poster="" id="video5" autoplay controls muted loop class="video-left">
              <source src="static/videos/flux/Yellow_flowers_swing_in_the_wind.mp4"
              type="video/mp4">
            </video>
            <video poster="" id="video6" autoplay controls muted loop class="video-right">
              <source src="static/videos/flux/Yellow_flowers_swing_in_the_wind_TTS.mp4"
              type="video/mp4">
            </video>
          </div>
          <h2 class="subtitle has-text-centered">
            Yellow flowers swing in the wind.
          </h2>
        </div>

        <div class="item">
          <div class="item-video">
            <video poster="" id="video5" autoplay controls muted loop class="video-left">
              <source src="static/videos/flux/a_cat_and_a_dog.mp4"
              type="video/mp4">
            </video>
            <video poster="" id="video6" autoplay controls muted loop class="video-right">
              <source src="static/videos/flux/a_cat_and_a_dog_TTS.mp4"
              type="video/mp4">
            </video>
          </div>
          <h2 class="subtitle has-text-centered">
            A cat and a dog.
          </h2>
        </div>

        <div class="item">
          <div class="item-video">
            <video poster="" id="video5" autoplay controls muted loop class="video-left">
              <source src="static/videos/flux/A_storm_trooper_vacuuming_the_beach.mp4"
              type="video/mp4">
            </video>
            <video poster="" id="video6" autoplay controls muted loop class="video-right">
              <source src="static/videos/flux/A_storm_trooper_vacuuming_the_beach_TTS.mp4"
              type="video/mp4">
            </video>
          </div>
          <h2 class="subtitle has-text-centered">
            A storm trooper vacuuming the beach.
          </h2>
        </div>
        
        <div class="item">
          <div class="item-video">
            <video poster="" id="video1" autoplay controls muted loop height="50%" class="video-left">
              <source src="static/videos/flux/A_panda_playing_on_a_swing_set.mp4"
              type="video/mp4">
            </video>
            <video poster="" id="video2" autoplay controls muted loop height="50%" class="video-right">
              <source src="static/videos/flux/A_panda_playing_on_a_swing_set_TTS.mp4"
              type="video/mp4">
            </video>
          </div>
          <h2 class="subtitle has-text-centered">
            A panda playing on a swing set.
          </h2>
        </div>

        <div class="item">
          <div class="item-video">
            <video poster="" id="video1" autoplay controls muted loop height="50%" class="video-left">
              <source src="static/videos/flux/a_zebra_and_a_giraffe.mp4"
              type="video/mp4">
            </video>
            <video poster="" id="video2" autoplay controls muted loop height="50%" class="video-right">
              <source src="static/videos/flux/a_zebra_and_a_giraffe_TTS.mp4"
              type="video/mp4">
            </video>
          </div>
          <h2 class="subtitle has-text-centered">
            A zebra and a giraffe.
          </h2>
        </div>

        <div class="item">
          <div class="item-video">
            <video poster="" id="video1" autoplay controls muted loop height="50%" class="video-left">
              <source src="static/videos/flux/Robot_dancing_in_Times_Square.mp4"
              type="video/mp4">
            </video>
            <video poster="" id="video2" autoplay controls muted loop height="50%" class="video-right">
              <source src="static/videos/flux/Robot_dancing_in_Times_Square_TTS.mp4"
              type="video/mp4">
            </video>
          </div>
          <h2 class="subtitle has-text-centered">
            Robot dancing in Times Square.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Spatial-MLLM Thinking Examples: </h2>
      <div style="display: flex">
        <div style="display: inline-block; width: 50%; padding-top: 20px; margin-bottom: -20px; text-align: center; font-size: 1.2em;">
          <strong><span style="color: rgb(4, 63, 129);">Left</span>: Original Video</strong>
        </div>
        <div style="display: inline-block; width: 50%; padding-top: 20px; margin-bottom: -20px; text-align: center; font-size: 1.2em;">
          <strong><span style="color: rgb(1, 95, 1);">Right</span>: Spatial-MLLM Thinking</strong>
        </div>
      </div>
      <div id="results-carousel" class="carousel results-carousel"> 
        <div class="item">
          <div class="item-video">
            <video id="video3" autoplay controls muted loop class="video-left" style="width: 50%;">
              <source src="static/videos/spatial-mllm-examples/appearance_order-5942004064.mp4" type="video/mp4">
            </video>

            <img src="static/images/basket-apporder.png" alt="Appearance Order Result" class="video-right" style="width: 50%; object-fit: contain;">
          </div>
          <h2 class="subtitle has-text-centered">
            Appearance Order
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Demos: Diffusion Models</h2>
      <div style="display: flex">
        <div style="display: inline-block; width: 50%; padding-top: 20px; margin-bottom: -20px; text-align: center; font-size: 1.2em;">
          <strong><span style="color: red;">Left</span>: Original Video</strong>
        </div>
        <div style="display: inline-block; width: 50%; padding-top: 20px; margin-bottom: -20px; text-align: center; font-size: 1.2em;">
          <strong><span style="color: green;">Right</span>: With Test-Time Scaling</strong>
        </div>
      </div>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <div class="item-video">
            <video poster="" id="video9" autoplay controls muted loop class="video-left">
              <source src="static/videos/cogvideo_5b/A_happy_fuzzy_panda_playing_guitar_nearby_a_campfire,_snow_mountain_in_the_background.mp4"
              type="video/mp4">
            </video>
            <video poster="" id="video10" autoplay controls muted loop class="video-right">
              <source src="static/videos/cogvideo_5b/A_happy_fuzzy_panda_playing_guitar_nearby_a_campfire,_snow_mountain_in_the_background_TTS.mp4"
              type="video/mp4">
            </video>
          </div>
          <h2 class="subtitle has-text-centered">
            A happy fuzzy panda playing guitar nearby a campfire, snow mountain in the background.
          </h2>
        </div>
        <div class="item">
          <div class="item-video">
            <video poster="" id="video11" autoplay controls muted loop class="video-left">
              <source src="static/videos/cogvideo_5b/A_corgi_is_playing_drum_kit.mp4"
              type="video/mp4">
            </video>
            <video poster="" id="video12" autoplay controls muted loop class="video-right">
              <source src="static/videos/cogvideo_5b/A_corgi_is_playing_drum_kit_TTS.mp4"
              type="video/mp4">
            </video>
          </div>
          <h2 class="subtitle has-text-centered">
            A corgi is playing drum kit.
          </h2>
        </div>
        
        <div class="item">
          <div class="item-video">
            <video poster="" id="video7" autoplay controls muted loop class="video-left">
              <source src="static/videos/cogvideo_5b/A_bigfoot_walking_in_the_snowstorm.mp4"
              type="video/mp4">
            </video>
            <video poster="" id="video8" autoplay controls muted loop class="video-right">
              <source src="static/videos/cogvideo_5b/A_bigfoot_walking_in_the_snowstorm_TTS.mp4"
              type="video/mp4">
            </video>
          </div>
          <h2 class="subtitle has-text-centered">
            A bigfoot walking in the snowstorm.
          </h2>
        </div>
        
        <div class="item">
          <div class="item-video">
            <video poster="" id="video7" autoplay controls muted loop class="video-left">
              <source src="static/videos/cogvideo_5b/A_panda_drinking_coffee_in_a_cafe_in_Paris.mp4"
              type="video/mp4">
            </video>
            <video poster="" id="video8" autoplay controls muted loop class="video-right">
              <source src="static/videos/cogvideo_5b/A_panda_drinking_coffee_in_a_cafe_in_Paris_TTS.mp4"
              type="video/mp4">
            </video>
          </div>
          <h2 class="subtitle has-text-centered">
            A panda drinking coffee in a cafe in Paris.
          </h2>
        </div>

        <div class="item">
          <div class="item-video">
            <video poster="" id="video7" autoplay controls muted loop class="video-left">
              <source src="static/videos/cogvideo_5b/A_person_is_writing.mp4"
              type="video/mp4">
            </video>
            <video poster="" id="video8" autoplay controls muted loop class="video-right">
              <source src="static/videos/cogvideo_5b/A_person_is_writing_TTS.mp4"
              type="video/mp4">
            </video>
          </div>
          <h2 class="subtitle has-text-centered">
            A person is writing.
          </h2>
        </div>

        <div class="item">
          <div class="item-video">
            <video poster="" id="video7" autoplay controls muted loop class="video-left">
              <source src="static/videos/cogvideo_5b/A_raccoon_is_playing_the_electronic_guitar.mp4"
              type="video/mp4">
            </video>
            <video poster="" id="video8" autoplay controls muted loop class="video-right">
              <source src="static/videos/cogvideo_5b/A_raccoon_is_playing_the_electronic_guitar_TTS.mp4"
              type="video/mp4">
            </video>
          </div>
          <h2 class="subtitle has-text-centered">
            A raccoon is playing the electronic guitar.
          </h2>
        </div>
        
        <div class="item">
          <div class="item-video">
            <video poster="" id="video7" autoplay controls muted loop class="video-left">
              <source src="static/videos/cogvideo_5b/A_steam_train_moving_on_a_mountainside.mp4"
              type="video/mp4">
            </video>
            <video poster="" id="video8" autoplay controls muted loop class="video-right">
              <source src="static/videos/cogvideo_5b/A_steam_train_moving_on_a_mountainside_TTS.mp4"
              type="video/mp4">
            </video>
          </div>
          <h2 class="subtitle has-text-centered">
            A steam train moving on a mountainside.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{liu2025video,
  title={Video-T1: Test-Time Scaling for Video Generation},
  author={Liu, Fangfu and Wang, Hanyang and Cai, Yimo and Zhang, Kaiyan and Zhan, Xiaohang and Duan, Yueqi},
  journal={arXiv preprint arXiv:2503.18942},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
